{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "jupytext": {
      "split_at_heading": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "name": "70_callback.wandb.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uYqS9FfQ5Ne",
        "outputId": "416fc3e9-4718-4dd8-e71e-50823073c16b"
      },
      "source": [
        "!pip install wandb"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.1-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 50.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.18-py3-none-any.whl (170 kB)\n",
            "\u001b[K     |████████████████████████████████| 170 kB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.0 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.7.4.3)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=136552d8fd2af872e6afbfe9843eb7e2053e3f3a659643dd052498908dc7b7a3\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=9efb2c3a45bc383847ff34ece88db271a3924c84fcaaec02de8274db47682260\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.18 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.3.1 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.12.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOZkI8JLSA9F"
      },
      "source": [
        "import wandb"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "bzUQOJsrSHyZ",
        "outputId": "4d613324-7273-47f4-af07-40475ecdf58e"
      },
      "source": [
        "wandb.login()\n",
        "\n",
        "#To use Weights & Biases without an account, you can call wandb.init(anonymous='allow')."
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "wandb: Paste an API key from your profile and hit enter: ··········\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "peBtrKM3RulM",
        "outputId": "49225f88-d00a-41be-bdc0-a08654302805"
      },
      "source": [
        "#hide\n",
        "#skip\n",
        "! [ -e /content ] && pip install -Uqq fastai  # upgrade fastai on colab"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 186 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 56 kB 4.0 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYKLU6tTQ5Ny"
      },
      "source": [
        "#all_slow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNLCFh2rQ5N5"
      },
      "source": [
        "#export\n",
        "from fastai.basics import *\n",
        "from fastai.callback.progress import *\n",
        "from fastai.text.data import TensorText\n",
        "from fastai.tabular.all import TabularDataLoaders, Tabular\n",
        "from fastai.callback.hook import total_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWX7sG1nQ5N_"
      },
      "source": [
        "#hide\n",
        "from nbdev.showdoc import *"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N22BJNYdQ5OG"
      },
      "source": [
        "#default_exp callback.wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3le2MlN4Q5OJ"
      },
      "source": [
        "# Wandb\n",
        "\n",
        "> Integration with [Weights & Biases](https://docs.wandb.com/library/integrations/fastai) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NNctAvrQ5OW"
      },
      "source": [
        "First thing first, you need to install wandb with\n",
        "```\n",
        "pip install wandb\n",
        "```\n",
        "Create a free account then run \n",
        "``` \n",
        "wandb login\n",
        "```\n",
        "in your terminal. Follow the link to get an API token that you will need to paste, then you're all set!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcCe6os_Q5Og"
      },
      "source": [
        "#export\n",
        "import wandb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aN7HYko6Q5Oq"
      },
      "source": [
        "#export\n",
        "class WandbCallback(Callback):\n",
        "    \"Saves model topology, losses & metrics\"\n",
        "    remove_on_fetch,order = True,Recorder.order+1\n",
        "    # Record if watch has been called previously (even in another instance)\n",
        "    _wandb_watch_called = False\n",
        "\n",
        "    def __init__(self, log=\"gradients\", log_preds=True, log_model=True, log_dataset=False, dataset_name=None, valid_dl=None, n_preds=36, seed=12345, reorder=True):\n",
        "        # Check if wandb.init has been called\n",
        "        if wandb.run is None:\n",
        "            raise ValueError('You must call wandb.init() before WandbCallback()')\n",
        "        # W&B log step\n",
        "        self._wandb_step = wandb.run.step - 1  # -1 except if the run has previously logged data (incremented at each batch)\n",
        "        self._wandb_epoch = 0 if not(wandb.run.step) else math.ceil(wandb.run.summary['epoch']) # continue to next epoch\n",
        "        store_attr('log,log_preds,log_model,log_dataset,dataset_name,valid_dl,n_preds,seed,reorder')\n",
        "\n",
        "    def before_fit(self):\n",
        "        \"Call watch method to log model topology, gradients & weights\"\n",
        "        self.run = not hasattr(self.learn, 'lr_finder') and not hasattr(self, \"gather_preds\") and rank_distrib()==0\n",
        "        if not self.run: return\n",
        "\n",
        "        # Log config parameters\n",
        "        log_config = self.learn.gather_args()\n",
        "        _format_config(log_config)\n",
        "        try:\n",
        "            wandb.config.update(log_config, allow_val_change=True)\n",
        "        except Exception as e:\n",
        "            print(f'WandbCallback could not log config parameters -> {e}')\n",
        "\n",
        "        if not WandbCallback._wandb_watch_called:\n",
        "            WandbCallback._wandb_watch_called = True\n",
        "            # Logs model topology and optionally gradients and weights\n",
        "            wandb.watch(self.learn.model, log=self.log)\n",
        "\n",
        "        # log dataset\n",
        "        assert isinstance(self.log_dataset, (str, Path, bool)), 'log_dataset must be a path or a boolean'\n",
        "        if self.log_dataset is True:\n",
        "            if Path(self.dls.path) == Path('.'):\n",
        "                print('WandbCallback could not retrieve the dataset path, please provide it explicitly to \"log_dataset\"')\n",
        "                self.log_dataset = False\n",
        "            else:\n",
        "                self.log_dataset = self.dls.path\n",
        "        if self.log_dataset:\n",
        "            self.log_dataset = Path(self.log_dataset)\n",
        "            assert self.log_dataset.is_dir(), f'log_dataset must be a valid directory: {self.log_dataset}'\n",
        "            metadata = {'path relative to learner': os.path.relpath(self.log_dataset, self.learn.path)}\n",
        "            log_dataset(path=self.log_dataset, name=self.dataset_name, metadata=metadata)\n",
        "\n",
        "        # log model\n",
        "        if self.log_model and not hasattr(self, 'save_model'):\n",
        "            print('WandbCallback requires use of \"SaveModelCallback\" to log best model')\n",
        "            self.log_model = False\n",
        "\n",
        "        if self.log_preds:\n",
        "            try:\n",
        "                if not self.valid_dl:\n",
        "                    #Initializes the batch watched\n",
        "                    wandbRandom = random.Random(self.seed)  # For repeatability\n",
        "                    self.n_preds = min(self.n_preds, len(self.dls.valid_ds))\n",
        "                    idxs = wandbRandom.sample(range(len(self.dls.valid_ds)), self.n_preds)\n",
        "                    if isinstance(self.dls,  TabularDataLoaders):\n",
        "                        test_items = getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[idxs]\n",
        "                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True, process=False)\n",
        "                    else:\n",
        "                        test_items = [getattr(self.dls.valid_ds.items, 'iloc', self.dls.valid_ds.items)[i] for i in idxs]\n",
        "                        self.valid_dl = self.dls.test_dl(test_items, with_labels=True)\n",
        "                self.learn.add_cb(FetchPredsCallback(dl=self.valid_dl, with_input=True, with_decoded=True, reorder=self.reorder))\n",
        "            except Exception as e:\n",
        "                self.log_preds = False\n",
        "                print(f'WandbCallback was not able to prepare a DataLoader for logging prediction samples -> {e}')\n",
        "\n",
        "    def after_batch(self):\n",
        "        \"Log hyper-parameters and training loss\"\n",
        "        if self.training:\n",
        "            self._wandb_step += 1\n",
        "            self._wandb_epoch += 1/self.n_iter\n",
        "            hypers = {f'{k}_{i}':v for i,h in enumerate(self.opt.hypers) for k,v in h.items()}\n",
        "            wandb.log({'epoch': self._wandb_epoch, 'train_loss': to_detach(self.smooth_loss.clone()), 'raw_loss': to_detach(self.loss.clone()), **hypers}, step=self._wandb_step)\n",
        "    \n",
        "    def log_predictions(self, preds):\n",
        "        inp,preds,targs,out = preds\n",
        "        b = tuplify(inp) + tuplify(targs)\n",
        "        x,y,its,outs = self.valid_dl.show_results(b, out, show=False, max_n=self.n_preds)\n",
        "        wandb.log(wandb_process(x, y, its, outs), step=self._wandb_step)\n",
        "\n",
        "    def after_epoch(self):\n",
        "        \"Log validation loss and custom metrics & log prediction samples\"\n",
        "        # Correct any epoch rounding error and overwrite value\n",
        "        self._wandb_epoch = round(self._wandb_epoch)\n",
        "        wandb.log({'epoch': self._wandb_epoch}, step=self._wandb_step)\n",
        "        # Log sample predictions\n",
        "        if self.log_preds:\n",
        "            try:\n",
        "                self.log_predictions(self.learn.fetch_preds.preds)\n",
        "            except Exception as e:\n",
        "                self.log_preds = False\n",
        "                self.remove_cb(FetchPredsCallback)\n",
        "                print(f'WandbCallback was not able to get prediction samples -> {e}')\n",
        "        wandb.log({n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}, step=self._wandb_step)\n",
        "\n",
        "    def after_fit(self):\n",
        "        if self.log_model:\n",
        "            if self.save_model.last_saved_path is None:\n",
        "                print('WandbCallback could not retrieve a model to upload')\n",
        "            else:\n",
        "                metadata = {n:s for n,s in zip(self.recorder.metric_names, self.recorder.log) if n not in ['train_loss', 'epoch', 'time']}\n",
        "                log_model(self.save_model.last_saved_path, metadata=metadata)                \n",
        "        self.run = True\n",
        "        if self.log_preds: self.remove_cb(FetchPredsCallback)\n",
        "        wandb.log({})  # ensure sync of last step\n",
        "        self._wandb_step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAezcTmbQ5PE"
      },
      "source": [
        "Optionally logs weights and or gradients depending on `log` (can be \"gradients\", \"parameters\", \"all\" or None), sample predictions if ` log_preds=True` that will come from `valid_dl` or a random sample pf the validation set (determined by `seed`). `n_preds` are logged in this case.\n",
        "\n",
        "If used in combination with `SaveModelCallback`, the best model is saved as well (can be deactivated with `log_model=False`).\n",
        "\n",
        "Datasets can also be tracked:\n",
        "* if `log_dataset` is `True`, tracked folder is retrieved from `learn.dls.path`\n",
        "* `log_dataset` can explicitly be set to the folder to track\n",
        "* the name of the dataset can explicitly be given through `dataset_name`, otherwise it is set to the folder name\n",
        "* *Note: the subfolder \"models\" is always ignored*\n",
        "\n",
        "For custom scenarios, you can also manually use functions `log_dataset` and `log_model` to respectively log your own datasets and models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IwPsA0EQ5PT"
      },
      "source": [
        "#export\n",
        "@patch\n",
        "def gather_args(self:Learner):\n",
        "    \"Gather config parameters accessible to the learner\"\n",
        "    # args stored by `store_attr`\n",
        "    cb_args = {f'{cb}':getattr(cb,'__stored_args__',True) for cb in self.cbs}\n",
        "    args = {'Learner':self, **cb_args}\n",
        "    # input dimensions\n",
        "    try:\n",
        "        n_inp = self.dls.train.n_inp\n",
        "        args['n_inp'] = n_inp\n",
        "        xb = self.dls.train.one_batch()[:n_inp]\n",
        "        args.update({f'input {n+1} dim {i+1}':d for n in range(n_inp) for i,d in enumerate(list(detuplify(xb[n]).shape))})\n",
        "    except: print(f'Could not gather input dimensions')\n",
        "    # other useful information\n",
        "    with ignore_exceptions():\n",
        "        args['batch size'] = self.dls.bs\n",
        "        args['batch per epoch'] = len(self.dls.train)\n",
        "        args['model parameters'] = total_params(self.model)[0]\n",
        "        args['device'] = self.dls.device.type\n",
        "        args['frozen'] = bool(self.opt.frozen_idx)\n",
        "        args['frozen idx'] = self.opt.frozen_idx\n",
        "        args['dataset.tfms'] = f'{self.dls.dataset.tfms}'\n",
        "        args['dls.after_item'] = f'{self.dls.after_item}'\n",
        "        args['dls.before_batch'] = f'{self.dls.before_batch}'\n",
        "        args['dls.after_batch'] = f'{self.dls.after_batch}'\n",
        "    return args"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_AsTXqpQ5PW"
      },
      "source": [
        "#export\n",
        "def _make_plt(img):\n",
        "    \"Make plot to image resolution\"\n",
        "    # from https://stackoverflow.com/a/13714915\n",
        "    my_dpi = 100\n",
        "    fig = plt.figure(frameon=False, dpi=my_dpi)\n",
        "    h, w = img.shape[:2]\n",
        "    fig.set_size_inches(w / my_dpi, h / my_dpi)\n",
        "    ax = plt.Axes(fig, [0., 0., 1., 1.])\n",
        "    ax.set_axis_off()\n",
        "    fig.add_axes(ax)\n",
        "    return fig, ax"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guM-OFUwQ5Pd"
      },
      "source": [
        "#export\n",
        "def _format_config_value(v):\n",
        "    if isinstance(v, list):\n",
        "        return [_format_config_value(item) for item in v]\n",
        "    elif hasattr(v, '__stored_args__'):\n",
        "        return {**_format_config(v.__stored_args__), '_name': v}\n",
        "    return v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pSNfryjgQ5Pj"
      },
      "source": [
        "#export\n",
        "def _format_config(config):\n",
        "    \"Format config parameters before logging them\"\n",
        "    for k,v in config.items():\n",
        "        if isinstance(v, dict):\n",
        "            config[k] = _format_config(v)\n",
        "        else:\n",
        "            config[k] = _format_config_value(v)\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Or13Cy93Q5Pr"
      },
      "source": [
        "#export\n",
        "def _format_metadata(metadata):\n",
        "    \"Format metadata associated to artifacts\"\n",
        "    for k,v in metadata.items(): metadata[k] = str(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ks1z25kmQ5Pt"
      },
      "source": [
        "#export\n",
        "def log_dataset(path, name=None, metadata={}, description='raw dataset'):\n",
        "    \"Log dataset folder\"\n",
        "    # Check if wandb.init has been called in case datasets are logged manually\n",
        "    if wandb.run is None:\n",
        "        raise ValueError('You must call wandb.init() before log_dataset()')\n",
        "    path = Path(path)\n",
        "    if not path.is_dir():\n",
        "        raise f'path must be a valid directory: {path}'\n",
        "    name = ifnone(name, path.name)\n",
        "    _format_metadata(metadata)\n",
        "    artifact_dataset = wandb.Artifact(name=name, type='dataset', metadata=metadata, description=description)\n",
        "    # log everything except \"models\" folder\n",
        "    for p in path.ls():\n",
        "        if p.is_dir():\n",
        "            if p.name != 'models': artifact_dataset.add_dir(str(p.resolve()), name=p.name)\n",
        "        else: artifact_dataset.add_file(str(p.resolve()))\n",
        "    wandb.run.use_artifact(artifact_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmBl1L85Q5Pu"
      },
      "source": [
        "#export\n",
        "def log_model(path, name=None, metadata={}, description='trained model'):\n",
        "    \"Log model file\"\n",
        "    if wandb.run is None:\n",
        "        raise ValueError('You must call wandb.init() before log_model()')\n",
        "    path = Path(path)\n",
        "    if not path.is_file():\n",
        "        raise f'path must be a valid file: {path}'\n",
        "    name = ifnone(name, f'run-{wandb.run.id}-model')\n",
        "    _format_metadata(metadata)    \n",
        "    artifact_model = wandb.Artifact(name=name, type='model', metadata=metadata, description=description)\n",
        "    with artifact_model.new_file(name, mode='wb') as fa:\n",
        "        fa.write(path.read_bytes())\n",
        "    wandb.run.log_artifact(artifact_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjKp70i6Q5Pw"
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def wandb_process(x:TensorImage, y, samples, outs):\n",
        "    \"Process `sample` and `out` depending on the type of `x/y`\"\n",
        "    res_input, res_pred, res_label = [],[],[]\n",
        "    for s,o in zip(samples, outs):\n",
        "        img = s[0].permute(1,2,0)\n",
        "        res_input.append(wandb.Image(img, caption='Input data'))\n",
        "        for t, capt, res in ((o[0], \"Prediction\", res_pred), (s[1], \"Ground Truth\", res_label)):\n",
        "            fig, ax = _make_plt(img)\n",
        "            # Superimpose label or prediction to input image\n",
        "            ax = img.show(ctx=ax)\n",
        "            ax = t.show(ctx=ax)\n",
        "            res.append(wandb.Image(fig, caption=capt))\n",
        "            plt.close(fig)\n",
        "    return {\"Inputs\":res_input, \"Predictions\":res_pred, \"Ground Truth\":res_label}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DaQ-nav7Q5Px"
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def wandb_process(x:TensorImage, y:(TensorCategory,TensorMultiCategory), samples, outs):\n",
        "    return {\"Prediction Samples\": [wandb.Image(s[0].permute(1,2,0), caption=f'Ground Truth: {s[1]}\\nPrediction: {o[0]}')\n",
        "            for s,o in zip(samples,outs)]}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pUMLLZgQ5P0"
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def wandb_process(x:TensorImage, y:TensorMask, samples, outs):\n",
        "    res = []\n",
        "    codes = getattr(y, 'codes', None)\n",
        "    class_labels = {i:f'{c}' for i,c in enumerate(codes)} if codes is not None else None\n",
        "    for s,o in zip(samples, outs):\n",
        "        img = s[0].permute(1,2,0)\n",
        "        masks = {}\n",
        "        for t, capt in ((o[0], \"Prediction\"), (s[1], \"Ground Truth\")):\n",
        "            masks[capt] = {'mask_data':t.numpy().astype(np.uint8)}\n",
        "            if class_labels: masks[capt]['class_labels'] = class_labels\n",
        "        res.append(wandb.Image(img, masks=masks))\n",
        "    return {\"Prediction Samples\":res}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qr48XM7yQ5P2"
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def wandb_process(x:TensorText, y:(TensorCategory,TensorMultiCategory), samples, outs):\n",
        "    data = [[s[0], s[1], o[0]] for s,o in zip(samples,outs)]\n",
        "    return {\"Prediction Samples\": wandb.Table(data=data, columns=[\"Text\", \"Target\", \"Prediction\"])}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxBNE4EMQ5P3"
      },
      "source": [
        "#export\n",
        "@typedispatch\n",
        "def wandb_process(x:Tabular, y:Tabular, samples, outs):\n",
        "    df = x.all_cols\n",
        "    for n in x.y_names: df[n+'_pred'] = y[n].values\n",
        "    return {\"Prediction Samples\": wandb.Table(dataframe=df)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUcHNjK5Q5P5"
      },
      "source": [
        "## Example of use:\n",
        "\n",
        "Once your have defined your `Learner`, before you call to `fit` or `fit_one_cycle`, you need to initialize wandb:\n",
        "```\n",
        "import wandb\n",
        "wandb.init()\n",
        "```\n",
        "To use Weights & Biases without an account, you can call `wandb.init(anonymous='allow')`.\n",
        "\n",
        "Then you add the callback to your `learner` or call to `fit` methods, potentially with `SaveModelCallback` if you want to save the best model:\n",
        "```\n",
        "from fastai.callback.wandb import *\n",
        "\n",
        "# To log only during one training phase\n",
        "learn.fit(..., cbs=WandbCallback())\n",
        "\n",
        "# To log continuously for all training phases\n",
        "learn = learner(..., cbs=WandbCallback())\n",
        "```\n",
        "Datasets and models can be tracked through the callback or directly through `log_model` and `log_dataset` functions.\n",
        "\n",
        "For more details, refer to [W&B documentation](https://docs.wandb.com/library/integrations/fastai)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oK9TGC6dQ5P7",
        "outputId": "682daf1e-cb2a-44ae-aa5b-a0c9396ad6a4"
      },
      "source": [
        "#hide\n",
        "#slow\n",
        "from fastai.vision.all import *\n",
        "import tempfile\n",
        "\n",
        "path = untar_data(URLs.MNIST_TINY)\n",
        "items = get_image_files(path)\n",
        "tds = Datasets(items, [PILImageBW.create, [parent_label, Categorize()]], splits=GrandparentSplitter()(items))\n",
        "dls = tds.dataloaders(after_item=[ToTensor(), IntToFloatTensor()])\n",
        "\n",
        "os.environ['WANDB_MODE'] = 'dryrun' # run offline\n",
        "with tempfile.TemporaryDirectory() as wandb_local_dir:\n",
        "    wandb.init(anonymous='allow', dir=wandb_local_dir)\n",
        "    learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), cbs=WandbCallback(log_model=False))\n",
        "    learn.fit(1)\n",
        "\n",
        "    # add more data from a new learner on same run\n",
        "    learn = cnn_learner(dls, resnet18, loss_func=CrossEntropyLossFlat(), cbs=WandbCallback(log_model=False))\n",
        "    learn.fit(1, lr=slice(0.005))\n",
        "    \n",
        "    # finish writing files to temporary folder\n",
        "    wandb.finish()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:3obm5ijh) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 294908<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/tmp/wandb/offline-run-20210107_084441-3obm5ijh/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/tmp/wandb/offline-run-20210107_084441-3obm5ijh/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path /tmp/tmphgwbs16r/wandb/ wasn't writable, using system temp directory\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /tmp/wandb/offline-run-20210107_084441-3obm5ijh\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:3obm5ijh). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Offline run mode, not syncing to the cloud.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` to enable cloud syncing.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.631549</td>\n",
              "      <td>0.243463</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>epoch</th>\n",
              "      <th>train_loss</th>\n",
              "      <th>valid_loss</th>\n",
              "      <th>time</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>0.638617</td>\n",
              "      <td>0.209739</td>\n",
              "      <td>00:02</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 295209<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find user logs for this run at: <code>/tmp/tmphupo103b/wandb/offline-run-20210107_084539-4lgubebi/logs/debug.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find internal logs for this run at: <code>/tmp/tmphupo103b/wandb/offline-run-20210107_084539-4lgubebi/logs/debug-internal.log</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run summary:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>2</td></tr><tr><td>train_loss</td><td>0.63862</td></tr><tr><td>raw_loss</td><td>0.42032</td></tr><tr><td>wd_0</td><td>0.01</td></tr><tr><td>sqr_mom_0</td><td>0.99</td></tr><tr><td>lr_0</td><td>0.0005</td></tr><tr><td>mom_0</td><td>0.9</td></tr><tr><td>eps_0</td><td>1e-05</td></tr><tr><td>wd_1</td><td>0.01</td></tr><tr><td>sqr_mom_1</td><td>0.99</td></tr><tr><td>lr_1</td><td>0.0005</td></tr><tr><td>mom_1</td><td>0.9</td></tr><tr><td>eps_1</td><td>1e-05</td></tr><tr><td>wd_2</td><td>0.01</td></tr><tr><td>sqr_mom_2</td><td>0.99</td></tr><tr><td>lr_2</td><td>0.005</td></tr><tr><td>mom_2</td><td>0.9</td></tr><tr><td>eps_2</td><td>1e-05</td></tr><tr><td>_step</td><td>21</td></tr><tr><td>_runtime</td><td>7</td></tr><tr><td>_timestamp</td><td>1610037956</td></tr><tr><td>valid_loss</td><td>0.20974</td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<h3>Run history:</h3><br/><style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    </style><table class=\"wandb\">\n",
              "<tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>▆▅▄▃▃▂▂▁▁▁▁█▆▄▄▃▂▂▂▂▁▁</td></tr><tr><td>raw_loss</td><td>▇▄▃▃▃▁▂▂▂▂▂█▅▃▄▂▂▁▃▃▁▂</td></tr><tr><td>wd_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_0</td><td>███████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mom_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_0</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_1</td><td>███████████▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>mom_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_1</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>wd_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>sqr_mom_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr_2</td><td>▁▁▁▁▁▁▁▁▁▁▁███████████</td></tr><tr><td>mom_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eps_2</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▁▁▄▅▅▅▅▅▅▇▇▇▇█</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▂▂▂▃▆▆▆▆▆▆▆▆▆▆█</td></tr><tr><td>valid_loss</td><td>█▁</td></tr></table><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /tmp/tmphupo103b/wandb/offline-run-20210107_084539-4lgubebi\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0G9eZLFQ5QA"
      },
      "source": [
        "#export\n",
        "_all_ = ['wandb_process']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34quoxTxQ5QB"
      },
      "source": [
        "## Export -"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbimvZk8Q5QC",
        "outputId": "1ee48420-e9c0-4c6b-e27d-ba5fea260e61"
      },
      "source": [
        "#hide\n",
        "from nbdev.export import *\n",
        "notebook2script()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converted 00_torch_core.ipynb.\n",
            "Converted 01_layers.ipynb.\n",
            "Converted 01a_losses.ipynb.\n",
            "Converted 02_data.load.ipynb.\n",
            "Converted 03_data.core.ipynb.\n",
            "Converted 04_data.external.ipynb.\n",
            "Converted 05_data.transforms.ipynb.\n",
            "Converted 06_data.block.ipynb.\n",
            "Converted 07_vision.core.ipynb.\n",
            "Converted 08_vision.data.ipynb.\n",
            "Converted 09_vision.augment.ipynb.\n",
            "Converted 09b_vision.utils.ipynb.\n",
            "Converted 09c_vision.widgets.ipynb.\n",
            "Converted 10_tutorial.pets.ipynb.\n",
            "Converted 10b_tutorial.albumentations.ipynb.\n",
            "Converted 11_vision.models.xresnet.ipynb.\n",
            "Converted 12_optimizer.ipynb.\n",
            "Converted 13_callback.core.ipynb.\n",
            "Converted 13a_learner.ipynb.\n",
            "Converted 13b_metrics.ipynb.\n",
            "Converted 14_callback.schedule.ipynb.\n",
            "Converted 14a_callback.data.ipynb.\n",
            "Converted 15_callback.hook.ipynb.\n",
            "Converted 15a_vision.models.unet.ipynb.\n",
            "Converted 16_callback.progress.ipynb.\n",
            "Converted 17_callback.tracker.ipynb.\n",
            "Converted 18_callback.fp16.ipynb.\n",
            "Converted 18a_callback.training.ipynb.\n",
            "Converted 18b_callback.preds.ipynb.\n",
            "Converted 19_callback.mixup.ipynb.\n",
            "Converted 20_interpret.ipynb.\n",
            "Converted 20a_distributed.ipynb.\n",
            "Converted 21_vision.learner.ipynb.\n",
            "Converted 22_tutorial.imagenette.ipynb.\n",
            "Converted 23_tutorial.vision.ipynb.\n",
            "Converted 24_tutorial.siamese.ipynb.\n",
            "Converted 24_vision.gan.ipynb.\n",
            "Converted 30_text.core.ipynb.\n",
            "Converted 31_text.data.ipynb.\n",
            "Converted 32_text.models.awdlstm.ipynb.\n",
            "Converted 33_text.models.core.ipynb.\n",
            "Converted 34_callback.rnn.ipynb.\n",
            "Converted 35_tutorial.wikitext.ipynb.\n",
            "Converted 36_text.models.qrnn.ipynb.\n",
            "Converted 37_text.learner.ipynb.\n",
            "Converted 38_tutorial.text.ipynb.\n",
            "Converted 39_tutorial.transformers.ipynb.\n",
            "Converted 40_tabular.core.ipynb.\n",
            "Converted 41_tabular.data.ipynb.\n",
            "Converted 42_tabular.model.ipynb.\n",
            "Converted 43_tabular.learner.ipynb.\n",
            "Converted 44_tutorial.tabular.ipynb.\n",
            "Converted 45_collab.ipynb.\n",
            "Converted 46_tutorial.collab.ipynb.\n",
            "Converted 50_tutorial.datablock.ipynb.\n",
            "Converted 60_medical.imaging.ipynb.\n",
            "Converted 61_tutorial.medical_imaging.ipynb.\n",
            "Converted 65_medical.text.ipynb.\n",
            "Converted 70_callback.wandb.ipynb.\n",
            "Converted 71_callback.tensorboard.ipynb.\n",
            "Converted 72_callback.neptune.ipynb.\n",
            "Converted 73_callback.captum.ipynb.\n",
            "Converted 97_test_utils.ipynb.\n",
            "Converted 99_pytorch_doc.ipynb.\n",
            "Converted dev-setup.ipynb.\n",
            "Converted index.ipynb.\n",
            "Converted quick_start.ipynb.\n",
            "Converted tutorial.ipynb.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3HQokb_Q5QE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}